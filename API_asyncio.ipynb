{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from urllib.parse import quote_plus\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import ast\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.asyncio import tqdm\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '7hzjsuigdkvndhjwhoqm8e2gbi0vnsnsdve3raaf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate time range\n",
    "- Fine-grained search from March 15, 2019 - March 15, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_range(start_date, end_date):\n",
    "    date_format = \"%m%d%Y\"\n",
    "    return [(date.strftime(date_format), date.strftime(date_format)) \n",
    "            for date in (start_date + timedelta(days=x) \n",
    "            for x in range((end_date - start_date).days + 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2019, 3, 15)\n",
    "end_date = datetime(2024, 3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ranges = generate_date_range(start_date, end_date)\n",
    "#date_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use async to fetch news data with progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_news(session, ticker, date_pair, page, API_KEY):\n",
    "    start_date, end_date = date_pair\n",
    "    url = f'https://stocknewsapi.com/api/v1?tickers={ticker}&type=article&date={start_date}-{end_date}&items=50&page={page}&extra-fields=id&token={API_KEY}'\n",
    "    async with session.get(url) as response:\n",
    "        if response.status == 200:\n",
    "            return await response.json()\n",
    "        else:\n",
    "            return {'data': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_news_for_ticker(ticker, date_ranges, API_KEY):\n",
    "    news_data = {}  \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_news(session, ticker, date_pair, page, API_KEY)\n",
    "                 for date_pair in date_ranges\n",
    "                 for page in range(1, 3)]\n",
    "        for task in tqdm.as_completed(tasks, desc=f\"Fetching {ticker}\"):\n",
    "            news_result = await task\n",
    "            if news_result['data']: \n",
    "                for article in news_result['data']:\n",
    "                    news_data[article['news_id']] = article\n",
    "    return news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(tickers, date_ranges, API_KEY):\n",
    "    all_ticker_news = {}\n",
    "    for ticker in tickers:\n",
    "        print(f\"Starting fetch for {ticker}\")\n",
    "        ticker_news = await fetch_news_for_ticker(ticker, date_ranges, API_KEY)\n",
    "        all_ticker_news[ticker] = ticker_news\n",
    "        print(f\"Completed fetching for {ticker}, Total news items: {len(ticker_news)}\")\n",
    "    return all_ticker_news\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment score calculation\n",
    "- Done at the end so we can easily modify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Score \n",
    "\n",
    "def calculate_sentiment_for_news(title, text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    title_text = title + \" \" + text\n",
    "    return analyzer.polarity_scores(title_text)\n",
    "\n",
    "def add_sentiment_to_news(all_ticker_news):\n",
    "    analyzer = SentimentIntensityAnalyzer()  \n",
    "    for ticker, articles in all_ticker_news.items():\n",
    "        for article_id, article in articles.items():\n",
    "            if 'title' in article and 'text' in article:\n",
    "                title_text = article['title'] + \" \" + article['text']\n",
    "                sentiment_scores = analyzer.polarity_scores(title_text)\n",
    "                article.update({\n",
    "                    'sentiment_neg': sentiment_scores['neg'],\n",
    "                    'sentiment_neu': sentiment_scores['neu'],\n",
    "                    'sentiment_pos': sentiment_scores['pos'],\n",
    "                    'sentiment_tot': sentiment_scores['compound']\n",
    "                })\n",
    "    return all_ticker_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fetch for LLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching LLY: 100%|██████████| 3656/3656 [00:50<00:00, 73.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for LLY, Total news items: 2750\n",
      "Starting fetch for UNH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching UNH: 100%|██████████| 3656/3656 [00:43<00:00, 84.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for UNH, Total news items: 1979\n",
      "Starting fetch for JNJ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching JNJ: 100%|██████████| 3656/3656 [00:46<00:00, 79.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for JNJ, Total news items: 4409\n",
      "Starting fetch for MRK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching MRK: 100%|██████████| 3656/3656 [00:44<00:00, 82.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for MRK, Total news items: 2994\n",
      "Starting fetch for ABBV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching ABBV: 100%|██████████| 3656/3656 [00:43<00:00, 83.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for ABBV, Total news items: 3067\n",
      "Starting fetch for MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching MSFT: 100%|██████████| 3656/3656 [00:44<00:00, 81.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for MSFT, Total news items: 12551\n",
      "Starting fetch for AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching AAPL: 100%|██████████| 3656/3656 [00:44<00:00, 82.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for AAPL, Total news items: 19979\n",
      "Starting fetch for NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching NVDA: 100%|██████████| 3656/3656 [00:43<00:00, 84.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for NVDA, Total news items: 10051\n",
      "Starting fetch for GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching GOOGL: 100%|██████████| 3656/3656 [00:44<00:00, 82.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for GOOGL, Total news items: 15058\n",
      "Starting fetch for AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching AMZN: 100%|██████████| 3656/3656 [00:45<00:00, 80.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for AMZN, Total news items: 21658\n",
      "Starting fetch for JPM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching JPM: 100%|██████████| 3656/3656 [00:45<00:00, 79.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for JPM, Total news items: 5124\n",
      "Starting fetch for V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching V: 100%|██████████| 3656/3656 [00:45<00:00, 80.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for V, Total news items: 3212\n",
      "Starting fetch for MA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching MA: 100%|██████████| 3656/3656 [00:47<00:00, 77.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for MA, Total news items: 2738\n",
      "Starting fetch for BAC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching BAC: 100%|██████████| 3656/3656 [00:47<00:00, 77.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for BAC, Total news items: 4134\n",
      "Starting fetch for WFC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching WFC: 100%|██████████| 3656/3656 [00:46<00:00, 78.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fetching for WFC, Total news items: 3397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'    \\nif __name__ == \"__main__\":\\n    API_KEY = \\'7hzjsuigdkvndhjwhoqm8e2gbi0vnsnsdve3raaf\\'\\n    #healthcare_stocks = [\"LLY\", \"UNH\", \"JNJ\", \"MRK\", \"ABBV\"]\\n    #technology_stocks = [\"MSFT\",\"AAPL\",\"NVDA\",\"GOOGL\",\"AMZN\"]\\n    #finance_stocks = [\"JPM\",\"V\",\"MA\",\"BAC\",\"WFC\"]\\n    #tickers = healthcare_stocks + technology_stocks + finance_stocks\\n    tickers = [\\'AMZN\\', \\'MSFT\\']\\n    #start_date = datetime(2019, 3, 15)\\n    #end_date = datetime(2024, 3, 15)\\n    start_date = datetime(2019, 3, 15)\\n    end_date = datetime(2024, 3, 16)\\n    date_ranges = generate_date_range(start_date, end_date)\\n    asyncio.run(main(tickers, date_ranges, API_KEY))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    API_KEY = '7hzjsuigdkvndhjwhoqm8e2gbi0vnsnsdve3raaf'\n",
    "    healthcare_stocks = [\"LLY\", \"UNH\", \"JNJ\", \"MRK\", \"ABBV\"]\n",
    "    technology_stocks = [\"MSFT\",\"AAPL\",\"NVDA\",\"GOOGL\",\"AMZN\"]\n",
    "    finance_stocks = [\"JPM\",\"V\",\"MA\",\"BAC\",\"WFC\"]\n",
    "    tickers = healthcare_stocks + technology_stocks + finance_stocks\n",
    "    start_date = datetime(2019, 3, 15)\n",
    "    end_date = datetime(2024, 3, 15)\n",
    "    date_ranges = generate_date_range(start_date, end_date)\n",
    "    news_data = asyncio.run(main(tickers, date_ranges, API_KEY))\n",
    "    news_data_with_sentiment = add_sentiment_to_news(news_data)\n",
    "    all_ticker_news = news_data_with_sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of articles fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLY: 2750 news articles\n",
      "UNH: 1979 news articles\n",
      "JNJ: 4409 news articles\n",
      "MRK: 2994 news articles\n",
      "ABBV: 3067 news articles\n",
      "MSFT: 12551 news articles\n",
      "AAPL: 19979 news articles\n",
      "NVDA: 10051 news articles\n",
      "GOOGL: 15058 news articles\n",
      "AMZN: 21658 news articles\n",
      "JPM: 5124 news articles\n",
      "V: 3212 news articles\n",
      "MA: 2738 news articles\n",
      "BAC: 4134 news articles\n",
      "WFC: 3397 news articles\n"
     ]
    }
   ],
   "source": [
    "for ticker, news_articles in all_ticker_news.items():\n",
    "    print(f\"{ticker}: {len(news_articles)} news articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_ticker_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check contents of resulting dictionary using async\n",
    "- Just practicing here how to access the dictionary structure\n",
    "- Displaying the first news article as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News for LLY:\n",
      "Title: House Committee Questions Eli Lilly, Sanofi, Novo Nordisk On 'Unacceptably High' Insulin Prices\n",
      "Text: House Energy & Commerce Committee leaders sent three letters to execs at Eli Lilly And Co (NYSE: LLY), Sanofi SA (NASDAQ: SNY), and Novo Nordisk A/S (NYSE: NVO), raising concerns that despite their supposed concern over the past two years with the price of insulin, the price still remains \"unacceptably high.\" Committee chair Frank Pallone (D-NJ) and subcommittee chair Diana DeGette (D-CO) note in all three letters that the price of insulin in the U.S. is more than 10 times that of 33 other countries.\n",
      "Date: Fri, 20 Aug 2021 14:34:33 -0400\n",
      "\n",
      "News for UNH:\n",
      "Title: The 3 Best Healthcare Stocks to Buy for June 2023\n",
      "Text: Healthcare stocks have been trailing the market this year. Vanguard's Health Care ETF (NYSEARCA: VHT ) is down 5% on the year, compared with a 10% year-to-date gain in the benchmark S&P 500 index.\n",
      "Date: Mon, 29 May 2023 15:13:25 -0400\n",
      "\n",
      "News for JNJ:\n",
      "Title: Insider Buying at These Biotech Firms Indicates Value\n",
      "Text: According to Wall Street legend, a young person approached one of the top businessmen in the U.S. (said to be J.P. Morgan, the founder of his namesake bank J.P.\n",
      "Date: Fri, 03 Sep 2021 11:18:01 -0400\n",
      "\n",
      "News for MRK:\n",
      "Title: Stability Seekers: 7 Low-Volatility Stocks for Uncertain Times\n",
      "Text: At first blush, focusing on low-volatility stocks may appear an overly pessimistic strategy. After all, the benchmark S&P 500 gained almost 8% in the trailing month.\n",
      "Date: Thu, 23 Nov 2023 16:02:53 -0500\n",
      "\n",
      "News for ABBV:\n",
      "Title: Buffett Loves These 4 High-Yield Dividend Aristocrats And So Will You\n",
      "Text: With the market 22% historically overvalued, many of the world's best blue-chips are trading at unattractive levels. In fact, Bank of America thinks stocks will barely deliver positive returns over the next decade.\n",
      "Date: Wed, 13 Oct 2021 21:48:29 -0400\n",
      "\n",
      "News for MSFT:\n",
      "Title: MarketWatch First Take: Are any tech companies immune to the economic damage from coronavirus?\n",
      "Text: Now that social-media companies, once the darlings of Wall Street, are confirming lower ad revenue due to the COVID-19 outbreak, investors are wondering if any companies in tech are immune from the pandemic’s economic damage.\n",
      "Date: Tue, 24 Mar 2020 23:15:52 -0400\n",
      "\n",
      "News for AAPL:\n",
      "Title: Top Tech Stocks To Buy Now? 2 To Watch\n",
      "Text: Tech stocks to watch this week.\n",
      "Date: Mon, 29 May 2023 20:34:05 -0400\n",
      "\n",
      "News for NVDA:\n",
      "Title: History Suggests the Nasdaq Will Surge in 2024: My Top 7 Artificial Intelligence (AI) Growth Stocks to Buy Before It Does\n",
      "Text: So far, the tech-heavy Nasdaq Composite index has always followed years in which it rebounded from a bear market with a second year of strong gains. Generative AI burst onto the scene last year, and it's still making waves.\n",
      "Date: Sat, 20 Jan 2024 16:00:00 -0500\n",
      "\n",
      "News for GOOGL:\n",
      "Title: 3 Stocks to Build Your Portfolio Around: Google, Pfizer & W.P. Carey\n",
      "Text: It needs a solid foundation, and Alphabet, Pfizer, and W.P. Carey have proven their worth over time.\n",
      "Date: Sat, 20 Jul 2019 09:00:00 -0400\n",
      "\n",
      "News for AMZN:\n",
      "Title: Amazon's cloud unit to create data centres, 1,000 jobs in New Zealand\n",
      "Text: Amazon.com Inc's cloud computing unit said on Wednesday it will launch data centres in New Zealand in 2024 and create about 1,000 jobs in the country over 15 years.\n",
      "Date: Wed, 22 Sep 2021 21:10:00 -0400\n",
      "\n",
      "News for JPM:\n",
      "Title: 3 Large-Cap Stocks to Buy After Earnings: JPM, NFLX and KMI\n",
      "Text: Waiting for earnings to pass can make a good deal of sense and in large-cap stocks JPM, NFLX and KMI it means being ready for a more secure purchase in your portfolio.\n",
      "Date: Fri, 18 Oct 2019 13:36:50 -0400\n",
      "\n",
      "News for V:\n",
      "Title: Visa: EPS Likely To Reach Turning Point And Grow 20% In Q3\n",
      "Text: Visa stock lost 2.7% during May and underperformed the S&P 500, but we believe earnings are at a turning point.\n",
      "Date: Tue, 01 Jun 2021 14:21:05 -0400\n",
      "\n",
      "News for MA:\n",
      "Title: What Soft Confidence? 3 ETFs & Stocks for Solid Holiday Buying\n",
      "Text: Americans??? confidence is dwindling as evident from the soft data for November.\n",
      "Date: Fri, 29 Nov 2019 14:30:00 -0500\n",
      "\n",
      "News for BAC:\n",
      "Title: Is This Warren Buffett Bank Stock Too Expensive?\n",
      "Text: This bank is one of Berkshire's top holdings, but are their better choices?\n",
      "Date: Tue, 14 Sep 2021 07:02:00 -0400\n",
      "\n",
      "News for WFC:\n",
      "Title: 2022 Q4 Earnings Season Preview: Will Estimates Hold Their Ground?\n",
      "Text: Many in the market consider current earnings expectations to be too high and fear that the Q4 earnings season will cause estimates to be lowered significantly.\n",
      "Date: Fri, 06 Jan 2023 18:48:22 -0500\n"
     ]
    }
   ],
   "source": [
    "for ticker, news_articles in all_ticker_news.items():\n",
    "    print(f\"\\nNews for {ticker}:\")\n",
    "    for article_id in list(news_articles)[:1]:\n",
    "        article = news_articles[article_id]\n",
    "        print(f\"Title: {article['title']}\\nText: {article['text']}\\nDate: {article['date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering: \n",
    "- Some financial news are associate with multiple tickers at a time. E.g tickers = ['GOOGL', 'NVDA', 'AAPL']\n",
    "    - e.g these type of news would be like \"Top 3 stocks in tech this month\", however we don't need these type of news, we want specific news to our chosen ticker\n",
    "- We want all of the financial news to be only associated for a specific ticker. E.g tickers = ['AAPL']\n",
    "- The code below loops over a specified target_tickers \n",
    "    - an extra requirement is done for 'GOOGL' stock because articles associated to this specific ticker comes with `['GOOG', 'GOOGL']` and I have to rename this just 'GOOGL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_news_by_tickers(news_dict, target_tickers):\n",
    "    filtered_dict = {}\n",
    "    for target_ticker in target_tickers:\n",
    "        ticker_key = tuple(target_ticker) if isinstance(target_ticker, list) else target_ticker\n",
    "        filtered_dict[ticker_key] = {}\n",
    "\n",
    "        for ticker, news_articles in news_dict.items():\n",
    "            if isinstance(target_ticker, list):\n",
    "                filtered_items = {\n",
    "                    article_id: item for article_id, item in news_articles.items() if set(item.get('tickers', [])) == set(target_ticker)\n",
    "                }\n",
    "            else:\n",
    "                filtered_items = {\n",
    "                    article_id: item for article_id, item in news_articles.items() if item.get('tickers') == [target_ticker]\n",
    "                }\n",
    "            filtered_dict[ticker_key].update(filtered_items)\n",
    "\n",
    "    return filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News for MSFT: 5934 articles\n",
      "News for AAPL: 12991 articles\n",
      "News for NVDA: 5179 articles\n",
      "News for ('GOOG', 'GOOGL'): 8316 articles\n",
      "News for AMZN: 14029 articles\n",
      "News for JPM: 2508 articles\n",
      "News for V: 1559 articles\n",
      "News for MA: 1512 articles\n",
      "News for BAC: 1757 articles\n",
      "News for WFC: 1810 articles\n",
      "News for LLY: 1693 articles\n",
      "News for UNH: 1049 articles\n",
      "News for JNJ: 2213 articles\n",
      "News for MRK: 1702 articles\n",
      "News for ABBV: 1452 articles\n"
     ]
    }
   ],
   "source": [
    "target_tickers = [\n",
    "    'MSFT', 'AAPL', 'NVDA', ['GOOG', 'GOOGL'], 'AMZN', 'JPM', 'V', 'MA', 'BAC', 'WFC', \n",
    "    'LLY', 'UNH', 'JNJ', 'MRK', 'ABBV'\n",
    "]\n",
    "\n",
    "filtered_news = filter_news_by_tickers(all_ticker_news, target_tickers)\n",
    "\n",
    "for ticker_key, articles in filtered_news.items():\n",
    "    print(f\"News for {ticker_key}: {len(articles)} articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for dataframe structure\n",
    "- Using the filtered news in the previous block, we loop over the dictionary to save the information we want\n",
    "- Rename `['GOOG', 'GOOGL']` to 'GOOGL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(news_items):\n",
    "    data = []\n",
    "    for ticker, news_articles in news_items.items():\n",
    "        display_ticker = 'GOOGL' if ticker == ('GOOG', 'GOOGL') else ticker\n",
    "        for article_id, news_item in news_articles.items():\n",
    "            data.append({\n",
    "                \"Date & Time\": news_item['date'],\n",
    "                \"Headline\": news_item['title'],\n",
    "                \"Text\": news_item['text'],\n",
    "                \"Source\": news_item['source_name'],\n",
    "                \"News ID\": article_id,\n",
    "                \"URL\": news_item['news_url'],\n",
    "                \"Ticker\": display_ticker, \n",
    "                \"Negative Sentiment Score\": news_item.get('sentiment_neg', 0),\n",
    "                \"Neutral Sentiment Score\": news_item.get('sentiment_neu', 0),\n",
    "                \"Positive Sentiment Score\": news_item.get('sentiment_pos', 0),\n",
    "                \"Total Sentiment Score (Compound)\": news_item.get('sentiment_tot', 0)\n",
    "            })\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_dataframe(filtered_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df_experiment = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if there are duplicates with news ids\n",
    "- we see there are none :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Date & Time, Headline, Text, Source, News ID, URL, Ticker, Negative Sentiment Score, Neutral Sentiment Score, Positive Sentiment Score, Total Sentiment Score (Compound)]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "duplicates = df_experiment[df_experiment.duplicated(subset='News ID', keep=False)]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = df_experiment.duplicated(subset='News ID').sum()\n",
    "print(duplicate_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the date and time an index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hg/khvl79693fx8p80bl9bp56wr0000gr/T/ipykernel_31749/3392456986.py:3: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_experiment['Date & Time'] = pd.to_datetime(df_experiment['Date & Time'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "df_experiment['Date & Time'] = pd.to_datetime(df_experiment['Date & Time'], errors='coerce')\n",
    "df_experiment.set_index('Date & Time', inplace=True)\n",
    "df_experiment.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save sorted df to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, 'data')\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "file_path = os.path.join(data_dir, 'news_data_2019_2024_sorted.csv')\n",
    "df_experiment.to_csv(file_path, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Erdos-DS-2024-newsworthy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
